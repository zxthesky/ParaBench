plm_dir: microsoft/deberta-v3-large
data_dir: Spico/TaskLAMA
# resume_from_checkpoint: outputs/debug
output_dir: outputs/debug_3000
cache_dir: resources/cache
overwrite_output_dir: true
max_seq_len: 512

per_device_train_batch_size: 8
per_device_eval_batch_size: 8
gradient_accumulation_steps: 1
num_train_epochs: 20
optim: adamw_torch
# adam_beta1: 0.9
# adam_beta2: 0.95
# weight_decay: 0.1
learning_rate: !!float 2e-5
warmup_ratio: 0.01
# warmup_steps: 0

evaluation_strategy: epoch
load_best_model_at_end: true
metric_for_best_model: eval_f1

seed: 1226
save_strategy: epoch
save_total_limit: 2
dataloader_num_workers: 0

logging_strategy: steps
logging_steps: 1
logging_first_step: true
report_to: tensorboard
log_level: info

do_train: true
do_eval: true
do_predict: true
